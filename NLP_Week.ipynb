{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab: Multiple LSTMs\n",
    "\n",
    "## Download and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "2024-03-06 15:55:35.604000: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-06 15:55:35.604122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-06 15:55:35.605786: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-06 15:55:37.516878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-06 15:55:39.541231: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as TFDS\n",
    "\n",
    "# Download the subword encoded pretokenized dataset\n",
    "dataset, info = TFDS.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Get the train and test splits\n",
    "train_data, test_data = dataset['train'], dataset['test']\n",
    "\n",
    "# Shuffle the training data\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Batch and pad the datasets to the maximum leangh of the sequences\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "timesteps (sequence length): 20\n",
      "features (embedding size): 16\n",
      "lstm output units: 8\n",
      "shape of input array: (1, 20, 16)\n",
      "shape of lstm output(return_sequences=True): (1, 20, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.layers import LSTM , Embedding, Dense, Bidirectional\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "timesteps = 20\n",
    "features = 16\n",
    "lstm_dim = 8\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'timesteps (sequence length): {timesteps}')\n",
    "print(f'features (embedding size): {features}')\n",
    "print(f'lstm output units: {lstm_dim}')\n",
    "\n",
    "# Define array input with random values\n",
    "random_input = np.random.rand(batch_size,timesteps,features)\n",
    "print(f'shape of input array: {random_input.shape}')\n",
    "\n",
    "# Define LSTM that returns a single output\n",
    "lstm_rs = LSTM(lstm_dim, return_sequences=True)\n",
    "result = lstm_rs(random_input)\n",
    "print(f\"shape of lstm output(return_sequences=True): {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM Neurlas](/home/alrashidi/Downloads/LSTM.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          523840    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 128)         66048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 635329 (2.42 MB)\n",
      "Trainable params: 635329 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm1_dim = 64\n",
    "lstm2_dim = 32\n",
    "dense_dim = 64\n",
    "\n",
    "# Build the model \n",
    "model = Sequential([\n",
    "    Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    Bidirectional(LSTM(lstm1_dim, return_sequences=True)),\n",
    "    Bidirectional(LSTM(lstm2_dim)),\n",
    "    Dense(dense_dim, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 16:56:32.602436: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 222298112 exceeds 10% of free system memory.\n",
      "2024-03-06 16:56:32.973214: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 222298112 exceeds 10% of free system memory.\n",
      "2024-03-06 16:56:32.973371: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 222298112 exceeds 10% of free system memory.\n",
      "2024-03-06 16:56:33.137011: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 222298112 exceeds 10% of free system memory.\n",
      "2024-03-06 16:56:44.067733: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 222298112 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/98 [..............................] - ETA: 35:32 - loss: 0.6938 - accuracy: 0.5020  "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "# Plot the accuracy and results \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlRashid",
   "language": "python",
   "name": "alrashid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
